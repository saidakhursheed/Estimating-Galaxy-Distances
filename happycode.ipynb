{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install optuna"
      ],
      "metadata": {
        "id": "-dw2vpUO_11p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS1viQ7095Jq",
        "outputId": "de25a20e-fc48-4166-9b74-8f6af5206b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-07-09 05:53:37,329] A new study created in memory with name: no-name-40a7dad3-5082-4ae3-8708-0f7e92d63d30\n",
            "[I 2024-07-09 05:58:10,750] Trial 0 finished with value: 0.010602612222224458 and parameters: {'model_type': 'RandomForest', 'n_estimators': 253, 'max_depth': 11}. Best is trial 0 with value: 0.010602612222224458.\n",
            "<ipython-input-3-ab25633483e3>:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
            "[I 2024-07-09 06:02:15,167] Trial 1 finished with value: 0.01550723495168882 and parameters: {'model_type': 'GradientBoosting', 'learning_rate': 0.10971161273986456, 'n_estimators': 75, 'max_depth': 32}. Best is trial 0 with value: 0.010602612222224458.\n",
            "<ipython-input-3-ab25633483e3>:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
            "[I 2024-07-09 06:05:22,514] Trial 2 finished with value: 0.016369350024180183 and parameters: {'model_type': 'GradientBoosting', 'learning_rate': 0.011611294071836748, 'n_estimators': 73, 'max_depth': 19}. Best is trial 0 with value: 0.010602612222224458.\n",
            "[I 2024-07-09 06:09:26,301] Trial 3 finished with value: 0.009759223069575685 and parameters: {'model_type': 'RandomForest', 'n_estimators': 129, 'max_depth': 31}. Best is trial 3 with value: 0.009759223069575685.\n",
            "[I 2024-07-09 06:13:20,311] Trial 4 finished with value: 0.009764346204159053 and parameters: {'model_type': 'RandomForest', 'n_estimators': 126, 'max_depth': 28}. Best is trial 3 with value: 0.009759223069575685.\n",
            "<ipython-input-3-ab25633483e3>:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
            "[I 2024-07-09 06:24:35,043] Trial 5 finished with value: 0.017540647200106053 and parameters: {'model_type': 'GradientBoosting', 'learning_rate': 0.04378205001300406, 'n_estimators': 205, 'max_depth': 45}. Best is trial 3 with value: 0.009759223069575685.\n",
            "<ipython-input-3-ab25633483e3>:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
            "[I 2024-07-09 06:32:16,824] Trial 6 finished with value: 0.010442637852396942 and parameters: {'model_type': 'GradientBoosting', 'learning_rate': 0.10579678876704617, 'n_estimators': 267, 'max_depth': 10}. Best is trial 3 with value: 0.009759223069575685.\n",
            "[I 2024-07-09 06:33:47,424] Trial 7 finished with value: 0.009883790357160974 and parameters: {'model_type': 'RandomForest', 'n_estimators': 51, 'max_depth': 25}. Best is trial 3 with value: 0.009759223069575685.\n",
            "<ipython-input-3-ab25633483e3>:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
            "[I 2024-07-09 06:47:18,773] Trial 8 finished with value: 0.014367786794684286 and parameters: {'model_type': 'GradientBoosting', 'learning_rate': 0.09973243484738642, 'n_estimators': 199, 'max_depth': 30}. Best is trial 3 with value: 0.009759223069575685.\n",
            "[I 2024-07-09 06:52:25,796] Trial 9 finished with value: 0.010486078740844012 and parameters: {'model_type': 'RandomForest', 'n_estimators': 282, 'max_depth': 12}. Best is trial 3 with value: 0.009759223069575685.\n",
            "[I 2024-07-09 06:56:54,312] Trial 10 finished with value: 0.00975182765393695 and parameters: {'model_type': 'RandomForest', 'n_estimators': 142, 'max_depth': 41}. Best is trial 10 with value: 0.00975182765393695.\n",
            "[I 2024-07-09 07:01:05,869] Trial 11 finished with value: 0.009760759761017106 and parameters: {'model_type': 'RandomForest', 'n_estimators': 132, 'max_depth': 41}. Best is trial 10 with value: 0.00975182765393695.\n",
            "[I 2024-07-09 07:05:27,917] Trial 12 finished with value: 0.009759031055325523 and parameters: {'model_type': 'RandomForest', 'n_estimators': 138, 'max_depth': 38}. Best is trial 10 with value: 0.00975182765393695.\n",
            "[I 2024-07-09 07:10:28,757] Trial 13 finished with value: 0.009733985056192882 and parameters: {'model_type': 'RandomForest', 'n_estimators': 158, 'max_depth': 39}. Best is trial 13 with value: 0.009733985056192882.\n",
            "[I 2024-07-09 07:16:00,046] Trial 14 finished with value: 0.009732339852523905 and parameters: {'model_type': 'RandomForest', 'n_estimators': 173, 'max_depth': 50}. Best is trial 14 with value: 0.009732339852523905.\n",
            "[I 2024-07-09 07:21:31,821] Trial 15 finished with value: 0.009729273041074005 and parameters: {'model_type': 'RandomForest', 'n_estimators': 175, 'max_depth': 49}. Best is trial 15 with value: 0.009729273041074005.\n",
            "[I 2024-07-09 07:28:40,158] Trial 16 finished with value: 0.009717969281936654 and parameters: {'model_type': 'RandomForest', 'n_estimators': 224, 'max_depth': 49}. Best is trial 16 with value: 0.009717969281936654.\n",
            "[I 2024-07-09 07:35:35,345] Trial 17 finished with value: 0.009715198351926585 and parameters: {'model_type': 'RandomForest', 'n_estimators': 217, 'max_depth': 48}. Best is trial 17 with value: 0.009715198351926585.\n",
            "[I 2024-07-09 07:43:08,880] Trial 18 finished with value: 0.009718105224558623 and parameters: {'model_type': 'RandomForest', 'n_estimators': 236, 'max_depth': 45}. Best is trial 17 with value: 0.009715198351926585.\n",
            "[I 2024-07-09 07:50:47,921] Trial 19 finished with value: 0.009722198083017915 and parameters: {'model_type': 'RandomForest', 'n_estimators': 228, 'max_depth': 35}. Best is trial 17 with value: 0.009715198351926585.\n",
            "[I 2024-07-09 08:00:07,413] Trial 20 finished with value: 0.009702739712832958 and parameters: {'model_type': 'RandomForest', 'n_estimators': 293, 'max_depth': 46}. Best is trial 20 with value: 0.009702739712832958.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/happy.csv')\n",
        "\n",
        "# Define feature columns and target column (using normalized features)\n",
        "features = ['feat1', 'feat2', 'feat3', 'feat4', 'feat5']\n",
        "target = 'z_spec'\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna(subset=features + [target])\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    model_type = trial.suggest_categorical('model_type', ['RandomForest', 'GradientBoosting'])\n",
        "    if model_type == 'RandomForest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
        "        max_depth = trial.suggest_int('max_depth', 10, 50)\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.2)\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
        "        max_depth = trial.suggest_int('max_depth', 10, 50)\n",
        "        model = GradientBoostingRegressor(\n",
        "            learning_rate=learning_rate,\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    return mse\n",
        "\n",
        "# Create a study and optimize the objective function\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "print(f'Best hyperparameters: {best_params}')\n",
        "\n",
        "# Train and evaluate the best model\n",
        "if best_params['model_type'] == 'RandomForest':\n",
        "    best_model = RandomForestRegressor(\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        max_depth=best_params['max_depth'],\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    best_model = GradientBoostingRegressor(\n",
        "        learning_rate=best_params['learning_rate'],\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        max_depth=best_params['max_depth'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "r2_train = best_model.score(X_train, y_train)\n",
        "\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "r2_test = best_model.score(X_test, y_test)\n",
        "\n",
        "print(f'Training MSE: {mse_train}')\n",
        "print(f'Training RMSE: {rmse_train}')\n",
        "print(f'Training R²: {r2_train}')\n",
        "print(f'Test MSE: {mse_test}')\n",
        "print(f'Test RMSE: {rmse_test}')\n",
        "print(f'Test R²: {r2_test}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TPE9bCiwAEWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}